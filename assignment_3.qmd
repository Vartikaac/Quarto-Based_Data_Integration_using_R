---
title: "MS5130 Assignment_3"
author: "Vartika Srivastava"
csl: "references_1/csl_1/apa-old-doi-prefix.csl"
format:
  html:
    embed-resources: true
    toc: true
    toc-expand: 2
    toc-title: Contents
    toc-depth: 3
---

# Welcome to Assignment 3 for MS5130: Advanced Analysis with R {style="color: black;"}

[***In this assignment, I will be showcasing my proficiency and the insights I've gained through my coursework in R programming. We will delve into complex statistical concepts, applying them to real-world data sets to demonstrate the power and versatility of R in solving analytical challenges. Join me as we explore the intricacies of data manipulation, visualization, and statistical modeling, highlighting the critical role of R in data science and statistical analysis***]{style="color: orange;"}

::: {.callout-note style="color: #555554;"}
# **Enhancements used in my Assignment are displayed below**ðŸ“¢

-   **(BE1) Executing the R code inside Quarto**

-   **(BE2) Use multiple datasets**

-   **(BE3) Combine datasets together**

-   **(BE4) Synergy of quantitative and qualitative analysis**

-   **(BE5) Explanatory text:**

-   **(SE1) Depict your data streams using Mermaid**

-   **(SE2) Use of a private GitHub repository**

-   **(SE3) Use of geographical data analysis using Leaflet**

-   **(SE4) Use of interactive charts/graphs/plots**
:::

## Selecting and Combining Datasets

The datasets mentioned are: the `amazon sale report`, which is a dataset from an earlier assignment (assignment1), and 2 new datasets I took for this assignment `Product_dataset` and `amazon_products`,

The code begins by loading three CSV files: '**Amazon Sale Report.csv**', '**Products.csv**', and '**Amazon-Products.csv**'. This is done using the read.csv function, with parameters set to ensure that the header row is recognized and string data is not automatically converted into factors.

For combining the datasets ,The inner_join function from the dplyr package is used to merge datasets based on common columns. This is a powerful feature for enhancing the analysis **(BE2)** because it allows for the integration of data from multiple sources showcasing **(BE3)** Combine datasets together, providing a more comprehensive view of the information.

First, Product_dataset is combined with amazon_sale_report using 'Category' as the key. This implies that the analysis is interested in exploring sales data within specific product categories. Then, the resulting dataset is further combined with amazon_products using 'index' as the key.

Our Final Dataset is **combined_Product_sale**

```{r message=FALSE, warning=FALSE }
#Load libraries
library(readr)
library(dplyr)

# Load the datasets
amazon_sale_report <- read.csv("dataset/Amazon Sale Report.csv", header = TRUE , stringsAsFactors = FALSE)
Product_dataset <- read.csv('dataset/Products.csv', header = TRUE , stringsAsFactors = FALSE)
amazon_products <- read.csv('dataset/Amazon-Products.csv', header = TRUE , stringsAsFactors = FALSE)

# Combine datasets

combined_Product_sale <- inner_join(Product_dataset, amazon_sale_report, by = "Category")

combined_sale_products <- inner_join(combined_Product_sale, amazon_products, by = "index")

#View the dataset
head(combined_sale_products)


```

::: {.callout-note collapse="true"}
Enhancements Used: BE1, BE2, BE3, BE5
:::

## Quantitative Analysis

### Generalized Linear Model

The code begins by importing the stats library, which is part of R's base packages and provides a broad array of statistical functions, including those needed for performing generalized linear models (GLM).

The pre-processing step involves converting the 'Status' variable into a binary format, where the status "Cancelled" is represented as 1, and all other statuses are represented as 0.

A GLM is then fitted to the pre-processed data, with the binary 'Cancelled' variable as the response and the numerical variables 'Amount' and 'Qty' (quantity) as predictors.

The glm function is used with the family = binomial argument, specifying that a logistic regression model is to be fitted. This type of model is chosen because the response variable is binary, and the goal is to understand how changes in the amount and quantity of orders relate to the likelihood of an order being cancelled. â€ƒ\
This analysis demonstrates **(BE4)**

```{r message=FALSE, warning=FALSE}
#import libraries
library(stats)

# Preprocess the data
# Convert the Status to a binary variable (1 for Cancelled, 0 otherwise)

combined_sale_products<- combined_sale_products %>%mutate(Cancelled = if_else(Status == "Cancelled", 1, 0))

# Fit a GLM 
glm_model <- glm(Cancelled ~ Amount + Qty, data = combined_sale_products, family = binomial)

# Summary of the model
summary(glm_model)


```

### Poisson GLM 

Switching gears to a Poisson generalized linear model (GLM) **demonstrating (SE5)**. This section of analysis aims to explore the relationship between the quantity of products ordered and the amount of those orders (Amount) using a Poisson regression model.

The Poisson GLM is particularly suited for modeling count data, where the response variable represents counts or numbers of events (in this case, the quantity of products ordered i.e. Amount)

```{r message=FALSE, warning=FALSE}

# Assuming Qty is  count response and Amount is a predictor

# Fit a Poisson GLM 
poisson_model <- glm(Qty ~ Amount, data = combined_sale_products, family = poisson())

# Summary of the model
summary(poisson_model)
```

### Gaussian GLM 

In this stage of analysis where I am demonstrating **(SE5)**, focus on modeling the relationship between the order amount (Amount), the quantity of products ordered (Qty), and the order status (Status) using a Gaussian generalized linear model (GLM).

Before fitting the model, I ensured that the Status variable is treated as a categorical factor by converting it with as.factor(). This is crucial because Status likely represents different categories of order statuses , and treating it as a factor allows the model to appropriately handle it as a nominal variable with discrete levels, rather than as a numeric variable. This model is aiming to predict the order amount based on the quantity of products ordered and the status of the order.

```{r message=FALSE, warning=FALSE}

# Convert Status to a factor if it's not already
combined_sale_products$Status <- as.factor(combined_sale_products$Status)

# Fit a Gaussian GLM 
gaussian_model <- glm(Amount ~ Qty + Status, data = combined_sale_products, family = gaussian())

# Summary of the model
summary(gaussian_model)
```

### Compare the Akaike Information Criterion (AIC) values for three different generalized linear models (GLMs) â€” Binomial, Poisson, and Gaussian

AIC is a measure of the relative quality of statistical models for a given set of data. Lower AIC values generally indicate a model that better fits the data

```{r message=FALSE, warning=FALSE}
library(ggplot2)

#Comparing models using AIC
aic_values <- data.frame(
  Model = c("Binomial", "Poisson", "Gaussian"),
  AIC = c(AIC(glm_model), AIC(poisson_model), AIC(gaussian_model))
)

# Plotting visualisation
ggplot(aic_values, aes(x = Model, y = AIC, fill = Model)) +
  geom_bar(stat = "identity") +
  labs(title = "Model AIC Comparison") +
  theme_minimal()
```

| The bar plot displays a comparison of the Akaike Information Criterion (AIC) values for three different generalized linear models: Binomial, Poisson, and Gaussian. Considering these AIC values, you would typically favor the Binomial model for further analysis and predictive tasks.

### Linear Regression Model

In the below code we are performing a Linear Regression Model to understand the impact of fulfillment status and the number of reviews on product ratings .

The initial steps involve preparing combined_sale_products dataset for analysis. This includes filtering out rows with missing values in specific columns (Rating, fulfilled1, and noreviews1) and converting fulfilled1 into a factor .

This interaction allows for examining how the relationship between the number of reviews (noreviews1) and the rating (Rating) changes based on the fulfillment status (fulfilled1). â€ƒ\
This analysis demonstrates **(BE4)**

```{r message=FALSE, warning=FALSE}


# Proceed with data cleaning and model creation
# Filter out rows with missing values in these columns and convert 'fulfilled1' to a factor
dataset_clean <- combined_sale_products %>%
  filter(!is.na(Rating), !is.na(fulfilled1), !is.na(noreviews1)) %>%
  mutate(fulfilled1 = as.factor(fulfilled1)) # Convert to factor for the interaction term

# Create a linear regression model with an interaction between fulfilled1 and noreviews1
model <- lm(Rating ~ fulfilled1 * noreviews1, data = combined_sale_products)

# Summary of the model
summary(model)

# Creating the interaction plot with ggplot2
ggplot(dataset_clean, aes(x = noreviews1, y = Rating, color = as.factor(fulfilled1))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, aes(group = fulfilled1)) + # Ensuring group aesthetic uses the factor
  scale_color_manual(values = c("blue", "red")) + # Manually specify colors for the factor levels
  labs(title = "Interaction Effect of Fulfillment and Number of Reviews on Rating",
       x = "Number of Reviews",
       y = "Rating",
       color = "Fulfilled by Platform") +
  theme_minimal() 
```

The plot depicts a flat, parallel trend between the number of reviews and product ratings for both platform-fulfilled and otherwise, suggesting that review volume has a negligible effect on ratings. Ratings cluster at the high end, indicating a general customer satisfaction or rating scale bias. The similarity in trends regardless of fulfillment status implies that how a product is fulfilled does not significantly influence its ratings.

::: {.callout-note collapse="true"}
Enhancements Used in Quantitative Analysis: BE1, BE4, BE5 ,SE5
:::

## Qualitative Analysis

### Text Mining

The code snippet demonstrates the use of text mining techniques in R by demonstrating **(BE4)** to process and visualize text data from a column, presumed to be 'ship-state', in the combined_sale_products dataset.

The text mining process here focuses on extracting insights from the 'ship-state' column, which contains text data related to shipping locations. By normalizing and pre-processing the text data, I remove irrelevant characters and standardize the text, ensuring that variations in formatting do not skew the analysis.

The resultant word cloud will be a visual representation of the frequency of each state within the data; the size of each word in the cloud corresponds to its frequency or importance in the corpus. It is a popular way to highlight the most prominent elements in textual data, providing an immediate visual summary of the text's content.

```{r message=FALSE, warning=FALSE}
library(readr)
library(dplyr)
library(tm)
library(wordcloud)
library(ggplot2)


#  'ship-state' is the column of interest
# Convert to lowercase for its consistency
combined_sale_products$ship.state <- tolower(combined_sale_products$ship.state)

# Create a text corpus
corpus <- Corpus(VectorSource(combined_sale_products$ship.state))

# Optional: preprocess the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)

# Generate the word cloud
wordcloud(corpus, max.words = 100, random.order = FALSE, rot.per = 0.35, colors=brewer.pal(8, "Dark2"))
```

The word cloud provides a visual representation of shipping locations from the dataset, with the size of each word reflecting its frequency. Larger words like "maharashtra," "uttarpradesh," and "telangana" indicate these states are common in the shipping . Smaller words represent less frequently occurring states. This visual summary quickly communicates which regions has maximum shipping and which regions has minimum shipping

::: {.callout-note collapse="true"}
Enhancements Used in Qualitative Analysis: BE1, BE4, BE5
:::

## Flow Diagram

> This Diagram outlines a detailed process involving loading datasets, combining them, pre-processing, fitting different models, comparing those models, and generating various plots for analysis.

```{mermaid}
flowchart TB
    load_datasets{Load Datasets} -->|read.csv| dataset1[Amazon Sale Report]
    load_datasets -->|read.csv| dataset2[Products]
    load_datasets -->|read.csv| dataset3[Amazon Products]
    dataset1 --> merge1[Combine Datasets]
    dataset2 --> merge1
    dataset3 --> merge1
    merge1 --> preprocess{Preprocess Data}
    preprocess -->|mutate| status_bin[Convert Status to Binary]
    status_bin --> fit_models{Fit Models}
    fit_models --> glm_model[GLM Binomial]
    fit_models --> poisson_model[Poisson GLM]
    fit_models --> gaussian_model[Gaussian GLM]
    glm_model --> compare{Compare Models}
    poisson_model --> compare
    gaussian_model --> compare
    compare -->|ggplot| aic_plot[AIC Comparison]
    preprocess -->|filter & mutate| clean_dataset[Clean Dataset for Additional Analysis]
    clean_dataset --> lm_model[Linear Regression Model]
    lm_model -->|ggplot| interaction_plot[Interaction Effect Plot]
    preprocess -->|aggregate| sales_over_time[Aggregate Sales Over Time]
    sales_over_time -->|plotly| sales_plot[Total Sales Over Time Plot]
    preprocess -->|gsub & as.numeric| ratings_hist[Convert Ratings to Numeric]
    ratings_hist -->|plotly| ratings_distribution[Histogram of Ratings Distribution]
    load_datasets -->|leaflet| map_visualization[Map Visualization]
    load_datasets -->|tm & wordcloud| word_cloud[Generate Word Cloud]
   
```

::: {.callout-note collapse="true"}
Enhancements Used: BE1, BE5, SE1
:::

## Shipping Destinations by State: An Overview of Indian Logistics

```{r message=FALSE, warning=FALSE}
library(leaflet)
library(dplyr)

# Define the states and their coordinates
states_coords <- data.frame(
  name = c("Andaman & Nicobar Islands", "Andhra Pradesh", "Arunachal Pradesh", "Assam", 
           "Bihar", "Chandigarh", "Chhattisgarh", "Dadra and Nagar Haveli", "Delhi", 
           "Gujarat", "Haryana", "Himachal Pradesh", "Jammu & Kashmir", "Jharkhand", 
           "Karnataka", "Kerala", "Ladakh", "Madhya Pradesh", "Maharashtra", "Manipur", 
           "Meghalaya", "Mizoram", "Nagaland", "Odisha", "Puducherry", "Punjab", 
           "Rajasthan", "Sikkim", "Tamil Nadu", "Telangana", "Tripura", "Uttar Pradesh", 
           "Uttarakhand", "West Bengal"),
  latitude = c(11.667025, 15.9129, 28.2180, 26.2006, 25.0961, 30.7333, 21.2787, 20.1809, 
               28.7041, 22.2587, 29.0588, 31.1048, 33.7782, 23.6102, 15.3173, 10.8505, 
               34.1526, 22.9734, 19.7515, 24.6637, 25.4670, 23.1645, 26.1584, 20.9517, 
               11.9416, 31.1471, 27.0238, 27.5330, 11.1271, 18.1124, 23.9408, 26.8467, 
               30.0668, 22.9868),
  longitude = c(92.735983, 79.7400, 94.7278, 92.9376, 85.3131, 76.7794, 81.8661, 73.0169, 
                77.1025, 71.1924, 76.0856, 77.1734, 76.5762, 85.2799, 75.7139, 76.2711, 
                77.5770, 78.6569, 75.7139, 93.9063, 91.3662, 92.9376, 94.5624, 85.0985, 
                79.8083, 75.3412, 74.2179, 88.5122, 78.6569, 79.0193, 91.9882, 80.9462, 
                79.0193, 87.8550)
)

# Initialize a Leaflet map
map <- leaflet() %>%
  addProviderTiles(providers$OpenStreetMap) %>%  # Add base map tiles
  setView(lng = 78.9629, lat = 20.5937, zoom = 5)  # Center the map on India

# Add markers for each state
for(i in 1:nrow(states_coords)) {
  map <- map %>%
    addMarkers(lng = states_coords$longitude[i], lat = states_coords$latitude[i],
               popup = states_coords$name[i])
}

# Display the map
map
```

The map illustrates the various shipping destinations across India

::: {.callout-note collapse="true"}
Enhancements Used: BE1, BE5, SE3
:::

## Interactive Graphs/Plots

### Graph 1

The code transforms a 'Date' column to the appropriate Date format and aggregates sales data by date to analyze trends over time. It then leverages plotly to create an interactive line chart.

```{r message=FALSE, warning=FALSE}

library(readr)
library(dplyr)
library(plotly)

# Convert Date column to Date type
combined_sale_products$Date <- as.Date(combined_sale_products$Date, format = "%m/%d/%Y")

# Aggregate sales by date
sales_over_time <- combined_sale_products %>%
  group_by(Date) %>%
  summarise(TotalSales = sum(Amount, na.rm = TRUE))

# 'sales_over_time' is the data frame with Date and TotalSales columns
plot <- plot_ly(data = sales_over_time, x = ~Date, y = ~TotalSales, type = 'scatter', mode = 'lines+markers') %>%
  layout(title = 'Total Sales Over Time',
         xaxis = list(title = 'Date'),
         yaxis = list(title = 'Total Sales (INR)'))

# Print the plot
plot

```

The graph depicts daily sales totals over time which can further help business analysis and decision-making based on sales patterns..

### Graph 2

The code helps to convert the colomn 'ratings' data by stripping out commas and non-numeric characters, then converts the cleansed strings to numeric values, readying them for analysis. Using Plotly, an interactive histogram is created to display the distribution of product ratings.

```{r}
# Convert ratings to numeric (after replacing commas and removing non-numeric characters)
combined_sale_products$ratings <- as.numeric(gsub(",", "", gsub("[^0-9.]", "", combined_sale_products$ratings)))

# Interactive Histogram of Ratings Distribution
plot <- plot_ly(data = amazon_products, x = ~ratings, type = "histogram") %>%
  layout(title = 'Distribution of Product Ratings',
         xaxis = list(title = 'Ratings'),
         yaxis = list(title = 'Count'))

# Print the plot
plot
```

This histogram allows for an intuitive exploration of the ratings' frequency, showing how often each rating occurs within the dataset. This visualization helps in understanding customer satisfaction levels

::: {.callout-note collapse="true"}
Enhancements Used : BE1, BE5, SE4
:::
